#which stays as uppercase marker, hence, a "normal" uppercase word is created which can be stemmed
text <- gsub("(\\b)([A-Z])([\\w]+)(\\b)", "\\1\\2\\L\\3\\4", text, perl=TRUE)
text <- text[text != ""]
return (text)
}
#FUNCTION Cleantext(text)-----------------------
##FUNCTION Uniquewords(text) ------------
Uniquewords <- function(text) {
text <- paste(unlist(text, recursive = TRUE), collapse = " ")
text <- unlist(strsplit(text, " "))
text <- sort(unique(text))
return (text)
}
##FUNCTION Uniquewords(text) ------------
##FUNCTION Loadstopwordslist.txt.csv(stopworddir_wfinalSlash)------------
#txt files may have comments marked via | or #
#csv files are assumed to have a HEADER and NO comments
#be aware of including the final slash
###REQUIRED FUNCTION Uniquewords(text)------------
if (grepl("^Uniquewords$", ls()) == FALSE) {
Uniquewords <- function(text) {
text <- paste(unlist(text, recursive = TRUE), collapse = " ")
text <- unlist(strsplit(text, " "))
text <- sort(unique(text))
return (text)
}
}
###REQUIRED FUNCTION Uniquewords(text)------------
Loadstopwordlist.txt.csv <- function(stopworddir_wfinalSlash) {
stopwordfiles <- list.files(stopworddir_wfinalSlash)
#initial the wordlist
stopwordlist <-  c("Dummy")
#for .txt files<<<<<<<<<<<<<<<<<<<<
txtfiles <- stopwordfiles[grep(".txt$", stopwordfiles)]
for (i in 1:length(txtfiles)) {
txtwordlist <- readLines(paste(stopworddir_wfinalSlash, txtfiles[i] ,sep=""))
txtwordlist <- gsub("\\|.*$", "", txtwordlist) #in case | is used as comment character
txtwordlist <- gsub("\\#.*$", "", txtwordlist) #in case # is used as comment character
txtwordlist <- gsub("^[[:blank:]]+|[[:blank:]]+$", "", txtwordlist)
txtwordlist <- txtwordlist[grep("[[:graph:]]", txtwordlist)]
stopwordlist <- c(stopwordlist, txtwordlist)
}
#for .txt files<<<<<<<<<<<<<<<<<<<<<<<<<
#for .csv files<<<<<<<<<<<<<<<<<<<<<<<
csvfiles <- stopwordfiles[grep(".csv$", stopwordfiles)]
for (i in 1:length(csvfiles)) {
csvwordlist <- read.csv(paste(stopworddir_wfinalSlash, csvfiles[i] ,sep=""), header = TRUE, sep=";")
csvwordlist <- Uniquewords(csvwordlist)
csvwordlist <- gsub("^[[:blank:]]+|[[:blank:]]+$", "", csvwordlist)
csvwordlist <- csvwordlist[grep("[[:graph:]]", csvwordlist)]
stopwordlist <- c(stopwordlist, csvwordlist)
}
#for .csv files<<<<<<<<<<<<<<<<<<<<<<<<<
stopwordlist <- unique(stopwordlist)
stopwordlist <- stopwordlist[grep("[[:graph:]]", stopwordlist)]
return(stopwordlist)
}
##FUNCTION Loadstopwordslist.txt.csv(stopworddir_wfinalSlash)------------
##FUNCTION Gsubstopwords <- function(stopwordlist, text) ------------
Gsubstopwords <- function(stopwordlist, text) {
for (i in seq(length(stopwordlist))) {
text <-  gsub(paste("(\\b)(",stopwordlist[i],")(\\b)", sep=""), "", text, perl = TRUE)
}
return(text)
}
##FUNCTION Gsubstopwords <- function(stopwordlist, text) ------------
##FUNCTION Stem vector---------------------------------
library("SnowballC")
Stem_vector <- function(vector) {
vector <- paste(vector, collapse=" ~c~ ")
vector <- unlist(strsplit(vector, " "))
#mark the uppercase words before stemming as some (like Umwelt) are stemmed to lower case
#and have to be reconverted to upper case after stemming
vector <- gsub(paste("(\\b)([A-Z|", special_german_characters[2], "|", special_german_characters[6],"|",special_german_characters[4],"])", sep=""), "X~\\2" , vector, perl=TRUE)
vector <- wordStem(vector, language = "german")
vector <- gsub("(X~)([\\w])", "\\U\\2" , vector, perl=TRUE)
vector <- paste(vector, collapse=" ")
vector <- unlist(strsplit(vector, " ~c~ "))
return (vector)
}
##FUNCTION Stem vector-----------------------------------
time.elapsed <- rbind(time.elapsed, proc.time())
row.names(time.elapsed)[nrow(time.elapsed)] <- "after_function_loading"
#END functions--------------------------------------------
#---------------------------04
####Purpose of the R code: import txt files into R list
#before running the following lines the text files have to be manipulated by including a predefined analysis code (see separate R code)
#this analysis code has to be adapted manually according to content of the txt file
#assumptions for filling the code:
#offset of deletion/extraction lines can be set, positive/negative/none, start the line with: !~~_OFF_-1_SET_~~!
#linebreak can be set, if end/startword is directly followed by a line break insert: !~~L1N3BR3AK~~!
#page range to be analyzed has to be set (by copying the start/stopword(s) at the respective location of the text into the code)
#single lines containing a certain structure (e.g. footnote) can be deleted
#on basis of xpdf file processing:
#page breaks are marked via: \f
#range to be analyzed should be defined in a way so that it inlcudes an \f in the beginning and at the end
###START - Settings and explanations------------------------
#assumed folder structure
#mainfolder/working directory
#mainfolder/documents -> collection of pdf documents to be analyzed
#mainfolder/text_files -> documents to be analyzed in desired txt format
#mainfolder/results -> final word lists, graphics, etc.
setwd(wd.source)
#setwd("C:\\Users\\Male\\Documents\\test")
#all text files are stored in a list with the following structure and //object types
#1 - [[DOCUMENT NAME]] //LIST
#2 - [[ELEMENT = NAME OF THE TEXT (here only a number, 1, name:1, 2: name:documentname)]] //LIST
#3 - [[ALLLINES = all lines of the text]] //CHARACTER
#4 - [[LINE = single line]] //CHARACTER
###END - Settings and explanations------------------------
#START read in data---------------------------------------
#read all txt files in folder and build a list of their names
files <- list.files(pattern="*.txt")
#initialize list
textlist <- vector(mode = "list", length = length(files))
for (f in 1:length(textlist)) {
text <- readLines(files[f], encoding = "UTF-8")
filename <- files[f]
names(textlist)[f] <- filename
#IMPORTANT: the first element assigned to a list should be a list element
#in case the list shall be extended later
#it seems that a data type assignment for the storage slot in the list is done somwhow, or at least of importance
#safe operation with lists is:
#1:initialize list
#2:store the first element as a list, e.g. for slot 5:
#2: list[[5]] <- list(elementone)
#3: all elements which shall be stored in the same slot (e.g. [[5]]) should also be stored as a list
#3: e.g. appending the list: list[[5]] <- c(list[[5]], list(elementtwo))
#hence there might be too many levels, but there should be no confusion regarding data handling this way
text <- list(text)
textlist[[f]] <- text
names(textlist[[f]]) <- filename
}
#END read in data----------------------------------------
#####Purpose of the R code: extraction of text blocks of similar structure (here single measures of CPCs) from several texts stored in a list
####Settings and explanations------------------------
##Assumption: the R code txtfilestoRlist is perforemd and txt files are stored within are as "textlist"
##<<<<<<<<<<<<<<<<<<<<further an analysis text code has to exist in the fileheads which was adapted manually (setting the words for extraction pattern) -> use code to write the code into the files (see "R/mainfolder/~")
##explanations for more settings (e.g. OFFSET) are explained in the code txt file directly
#list how text documents are stored has the following levels and //classes
#0 - [[DOCUMENT NAME]] //LIST
#1 - [[ELEMENT = NAME OF THE TEXT (here only a number, 1=wholetext, 2=available_pattern, ...)]] //LIST
#2 - [[ALLLINES = all lines of the text]] //CHARACTER
#3 - [[LINE = single line]] //CHARACTER
#####Settings and explanations------------------------
#textlist <- interim2
#START check the availability of potential extraction patterns by number of occurences of potential extraction words--------------------------
#loop through all elements in the textlist
for (t in 1:length(textlist)) {  #outer loop - all files
#t=1   #for test runs
#print(names(textlist)[[t]]) #for test runs
#read the text from the list class()=character
wholetext <- textlist[[t]][[1]]
##START reading the anaylsis code----------------------------------
codestart <- grep("!Analysis_Code_Start!",wholetext)
codeend <- grep("!Analysis_Code_End!",wholetext)
code <- wholetext[codestart:codeend]
#reading in the pages to be analyzed
startpage <- code[grep("pagerange",code)+1]
endpage <- code[grep("pagerange",code)+2]
#number of lines to be read in from the code containing words for pattern matching
desiredpatternrange <- grep("desired_pattern_end",code)-grep("desired_pattern_start",code)-1
#extracted vector of words for further processing / pattern matching
desiredpattern <- code[grep("desired_pattern_start",code)+rep(1:desiredpatternrange)]
##END reading the analysis code-------------------------------------
##START check availability of pattern within the text----------------------------------
#delete code from the text
wholetext <- wholetext[-c(codestart:codeend)]
#select the desired page range
measurestext <- wholetext[grep(startpage,wholetext):grep(endpage,wholetext)]
#check the number of occurence of potential pattern extraction words and build a matrix: word : number of occurences
availablepattern <- c(sapply(1:length(desiredpattern), function(i) {
#allinterimlines <- grep(startextract[e], measuretext)
sum(grepl(desiredpattern[i], measurestext))
}))
pattern <- cbind(desiredpattern, availablepattern)
pattern <- list(pattern)
names(pattern) <- paste("pattern_", names(textlist[t]), sep="")
textlist[[t]] <- c(textlist[[t]],pattern)
##END check availability of pattern within the text----------------------------------
} #end of outer loop through all meta list elements
#END check the availability of potential extraction patterns by number of occurences of potential extraction words--------------------------
#START Display the availability of pattern for post-processing of the pattern matching and extraction----------------
sapply(c(1:length(textlist)),  function(i) {
textlist[[i]][2]
})
#END Display the availability of pattern for post-processing of the pattern matching and extraction----------------
print("Please identify suitable extraction patterns manually and store them in a txt file in the format: 'c(10,22)  | name_of_document_to_be_analyzed.txt'")
#check the availability of patterns manually and select suitable start and end strings for the extraction
#the identified pattern of two words for a specific document have to be written manually into the following text file:
#M:\Science\Promotion\Working_Documents\R\niedersachsen_city_evaluation\CPC_evaluation_of_measures\identified_extraction_patterns.txt
#the format has to be as follows:
#       c(10,22)  | z_layout_GER_Hildesheim_CPC1_main_region_catalogue_of_measures_UTF8.txt
#the numbers of the vector represent the vector element numbers of the start/end string in the pattern list of a document
#-----------------------05
#####Append code at filehead in each file by identified patterns
#extraction patterns are stored in:
identifiedpatterns <- paste(wd.interim,"identified_extraction_patterns.txt", sep="")
#in the format
#       c(10,22)  | z_layout_GER_Hildesheim_CPC1_main_region_catalogue_of_measures_UTF8.txt
#the numbers of the vector represent the vector element numbers of the start/end string in the pattern list of a document
identifiedpatterns <-  readLines(identifiedpatterns)
#clean text from comments
identifiedpatterns <- gsub("#.*$", "", identifiedpatterns)
identifiedpatterns <- identifiedpatterns[identifiedpatterns != ""]
identifiedpatterns <- strsplit(identifiedpatterns, "\\|")
identifiedpatterns <- lapply(identifiedpatterns, function(x) gsub("[[:blank:]]+$|^[[:blank:]]+","", x))
for (i in 1:length(identifiedpatterns )) {#loop through the identifiedpatterns to write them in the fileheads
#check which identified pattern refers to which textlist element
#and if a suitable element exists at all
#e.g. in case the order of documents changed
extractionpattern <- eval(parse(text=identifiedpatterns[[i]][[1]]))
patternidintextlist <- grep(identifiedpatterns[[i]][[2]], names(textlist), fixed = TRUE)
if (sum(patternidintextlist) > 0) { #check if the filename of the stored pattern matches the one in the textlist, if yes, insert extraction pattern
startextractstring <- textlist[[patternidintextlist]][[2]][[extractionpattern[1],1]]
textlist[[patternidintextlist]][[1]] <- append(textlist[[patternidintextlist]][[1]], startextractstring, after = grep("startextract \\|", textlist[[patternidintextlist]][[1]]))
endextractstring <- textlist[[patternidintextlist]][[2]][[extractionpattern[2],1]]
textlist[[patternidintextlist]][[1]] <- append(textlist[[patternidintextlist]][[1]], endextractstring, after = grep("endextract \\|", textlist[[patternidintextlist]][[1]]))
} else { #in case the filename of the extraction pattern is not matched in the textlist
print(paste("Not matched in textlist:", identifiedpatterns[[i]][[2]]))
notmatched <- TRUE
}
} #end loop through identifiedpatterns
#-------------------------------------06
######Purpose of the R code: extraction of text blocks of similar structure (here single measures of CPCs) from several texts stored in a list
####Settings and explanations------------------------
#analysis code has to exist in the files -> use code to write the code into the files (see "R/mainfolder/~")
#it has to be adapted manually according to content of the txt file
#assumptions for filling the code:
#offset of deletion/extraction lines can be set, positive/negative/none, start the line with: !~~_OFF_-1_SET_~~!
#start and end of page range which shall be considered have to be inserted manually,
#by copying the start/stopword(s) at the respective location of the text into the code
#page breaks are marked via: \f
#list which has been generated in previous step has the following levels and //classes
#0 - [[DOCUMENT NAME]] //LIST
#1 - [[ELEMENT = NAME OF THE TEXT (here only a number, 1=wholetext, 2=available pattern, ...)]] //LIST
#2 - [[ALLLINES = all lines of the text]] //CHARACTER
#3 - [[LINE = single line]] //CHARACTER
####Settings and explanations------------------------
#START extract the measures from each single text-------------------
#loop through all elements in the textlist
for (t in 1:length(textlist)) {  #outer for loop - all files
#activate for test runs<<<<<<<<<
#t=  2
#print(names(measurelist[t]))
# print(names(textlist)[t])
wholetext <- textlist[[t]][[1]]
###START reading the anaylsis code-------------------------------------
#the code has been changed manually in the texts and is read in again
codestart <- grep("!Analysis_Code_Start!",wholetext)
codeend <- grep("!Analysis_Code_End!",wholetext)
code <- wholetext[codestart:codeend]
#separate code from the text
wholetext <- wholetext[-c(codestart:codeend)]
####START check for marker words F~A~L~S~E and T~R~U~E in text------------------------------
#the words F~A~L~S~E and T~R~U~E are used in the script at some parts instead of the boolean TRUE/FALSE
#as the latter might in some steps (e.g. by gsub) be converted to character and not fulfill their purpose
#in order to avoid mistakes in processing the text, it may not contain these words, this is checked
if (sum(grepl("F~A~L~S~E", wholetext)) > 0) {
wholetext <- gsub("F~A~L~S~E", "FALSE", wholetext)
print(paste("The string F~A~L~S~E was replaced by FALSE in: ", names(textlist)[t], sep=""))
}
if (sum(grepl("T~R~U~E", wholetext)) > TRUE) {
wholetext <- gsub("T~R~U~E", "TRUE", wholetext)
print(paste("The string T~R~U~E was replaced by TRUE in: ", names(textlist)[t], sep=""))
}
####END check for marker words F~A~L~S~E and T~R~U~E in text--------------------------
#reading in the pages to be analyzed
startpage <- code[grep("pagerange",code)+1]
endpage <- code[grep("pagerange",code)+2]
#to clean the text from footnotes, etc.
if (grep("single_line_deletion_end",code)-grep("single_line_deletion_start",code)-1 >= 1) {
deletesinglelinesrange <- grep("single_line_deletion_end",code)-grep("single_line_deletion_start",code)-1
deletesinglelines <- code[grep("single_line_deletion_start",code)+rep(1:deletesinglelinesrange)]
} else {
deletesinglelines <- "F~A~L~S~E"
}
#read in extraction pattern
#if single string pattern (FROM X TO X TO X TO END) option is set in the first line of extraction pattern take this inot account
#or otherwise use two (or more) extraction words
if (grepl("S~I~N~G~L~E~S~T~R~I~N~G~P~A~T~T~E~R~N:", code[(grep("startextract",code)+1)]) == TRUE) {
startextract <- gsub("S~I~N~G~L~E~S~T~R~I~N~G~P~A~T~T~E~R~N:", "", code[(grep("startextract",code)+1)] )
endextract <- "F~A~L~S~E"
singlestringpattern <- "T~R~U~E"
} else {
#defining the numbers of words and lines for extraction / deletion
extractrange <- grep("endextract",code)-grep("startextract",code)-1
#defining end/start words based on above ranges to be extracted or deleted - stored in vectors
startextract <- code[grep("startextract",code)+rep(1:extractrange)]
endextract <- code[grep("endextract",code)+rep(1:extractrange)]
singlestringpattern <- "F~A~L~S~E"
}
####START read in offset for extraction lines---------------------------------------
#!~~_OFF_0_SET_~~!
#1 Offset
offsetstartextract <- c(sapply(1:length(startextract), function(i) {
if (grepl("!~~_OFF_-?[[:digit:]]_SET_~~!", startextract[i]) == FALSE) {
0
}
else {
#regexpr searches for an expression and tells if and where in a given character vector it is
#regmatches finally extracts the data which matches according to regexpr
as.numeric(regmatches(substr(startextract[i],1,14),regexpr("-?[[:digit:]]+", substr(startextract[i],1,14))))
}
}))
#2 Offset
if (endextract[1] == "F~A~L~S~E") { #first if - here the case of single string patterns has to be taken into account
offsetendextract <- 0
} else { #first else
offsetendextract <- c(sapply(1:length(endextract), function(i) {
if (grepl("!~~_OFF_-?[[:digit:]]_SET_~~!", endextract[i]) == FALSE) { #second if
0
}
else { #second else
as.numeric(regmatches(substr(endextract[i],1,14),regexpr("-?[[:digit:]]+", substr(endextract[i],1,14))))
} #end second if
}))
} #end first if
####END read in offset for extraction lines---------------------------------------
####START clean the words of the code from steering settings such as line break settings-------------------
startextract <- gsub("!~~.*~~!", "", startextract)
endextract <- gsub("!~~.*~~!", "", endextract)
deletesinglelines <- gsub("!~~.*~~!", "", deletesinglelines)
####END clean the words of the code from steering settings such as line break settings-------------------
###END reading the anaylsis code-------------------------------------
###START extract page range--------------------------
measuretext <- wholetext[grep(startpage,wholetext):grep(endpage,wholetext)]
###END extract page range--------------------------
###START find the lines to START / END extraction on basis of a search string / search cases-------------------
startlines <- c(sapply(seq(length(startextract)), function(e) {
grep(startextract[e], measuretext)+offsetstartextract[e]
})) #end sapply loop
#for checking the output connected to the lines, use this command: measuretext[startlines]
if (singlestringpattern == "T~R~U~E") {
#all rows to be extracted from measuretext
#set endlines for extraction from one element to the next and last element = last row
endlines <- startlines
#delete the first entry, hence extraction would start with startlines[1] and end with startlines[2]
endlines <- endlines[-1]
#however extraction should be only until stratlines[2]-1 therfore -1 on all rows
endlines <- endlines-1
#as the final element is missing now the end of the text is defined as last endextraction line
endlines <- c(endlines, length(measuretext)) #add the last row as last entry
} else {
endlines <- c(sapply(seq(length(endextract)), function(e) {
grep(endextract[e], measuretext)+offsetendextract[e]
})) #end sapply loop
} #endif
#all rows to be extracted from measuretext
extractall <- cbind(startlines, endlines)
###START find the lines to START / END extraction on basis of a search string / search cases---------------------
###START extract single text parts of the selected page range and store them in a matrix-------------
#explanation of the code - see deletall block
#the extraction lines are fixed now therefore the extraction patterns (as single words) can be cleaned from the text
#for singleline pattern there is only startextract which can be deleted
if (singlestringpattern == "T~R~U~E") {
measuretext <- gsub( startextract ,"",measuretext)
} else {
measuretext <- gsub( startextract ,"",measuretext)
measuretext <- gsub( endextract ,"",measuretext)
}
if (singlestringpattern == "T~R~U~E") { #first if FALSE in the case of SINGLELINEPATTERN
#build a vector with each element including a single measure - collapsed text from the rows to be extracted
measuresextracted <- c(sapply(seq(nrow(extractall)), function(x) {
rows <- (extractall[x,1]:extractall[x,2])
measure <- measuretext[rows] #extracted text
#for the current analysis it makes sense to collapse the measure into plain text
#it might also be stored as a list
#in case the former structure shall be generated again the wording " ~~collapse~~ " is included at the collaped locations
####START delete single lines--------------------------
if (deletesinglelines[1] == "F~A~L~S~E") {  #first if/else
# print(paste("no deletelines, nothing deleted in: ", names(textlist[t])))
} else if  (deletesinglelines[1] != "F~A~L~S~E") { #if there is something to delete, loop through the vector of lines containing the deletion marker string
for (z in 1:length(deletesinglelines)) { #for loop through deletesinglelines
if (sum(c(grep(deletesinglelines[z], measure))) == 0) {  # second first/ifelse  / in case matching for deletion fails and a integer(o) vector is returned, a safety net is simply not to delete
# if (deletesinglelines[z] != "!~delete_line_~!") { #only print something if the deletline text is not a the dummy text
#  print(paste("failed to match deleteline", deletesinglelines[z] , "nothing deleted in: ", names(textlist[t])))
# }
} else if (sum(c(grep(deletesinglelines[z], measure))) > 0) {
measure <- measure[-c(grep(deletesinglelines[z], measure))]
} #end second if/else
} #end for loop through deletesinglelines
} #end first if/else
####END delete single lines--------------------------
paste(measure, collapse= " ~~collapse~~ ")
}))
} else {
#check if number of start/endextractlines are equal------------------
if (length(endlines) != length(startlines)) {
print(paste("ERROR - number of start/endextractlines not equal in", names(textlist[t])))
stop}
#build a vector with each element including a single measure - collapsed text from the rows to be extracted
measuresextracted <- c(sapply(seq(nrow(extractall)), function(x) {
rows <- (extractall[x,1]:extractall[x,2])
measure <- measuretext[rows] #extracted text
#for the current analysis it makes sense to collapse the measure into plain text
#it might also be stored as a list
#in case the former structure shall be generated again the wording " ~~collapse~~ " is included at the collaped locations
####START delete single lines--------------------------
if (deletesinglelines[1] == "F~A~L~S~E") {  #first if/else
# print(paste("no deletelines, nothing deleted in: ", names(textlist[t])))
} else if  (deletesinglelines[1] != "F~A~L~S~E") { #if there is something to delete, loop through the vector of lines containing the deletion marker string
for (z in 1:length(deletesinglelines)) { #for loop through deletesinglelines
if (sum(c(grep(deletesinglelines[z], measure))) == 0) {  # second first/ifelse  / in case matching for deletion fails and a integer(o) vector is returned, a safety net is simply not to delete
# if (deletesinglelines[z] != "!~delete_line_~!") { #only print something if the deletline text is not a the dummy text
#  print(paste("failed to match deleteline", deletesinglelines[z] , "nothing deleted in: ", names(textlist[t])))
# }
} else if (sum(c(grep(deletesinglelines[z], measure))) > 0) {
measure <- measure[-c(grep(deletesinglelines[z], measure))]
} #end second if/else
} #end for loop through deletesinglelines
} #end first if/else
####END delete single lines--------------------------
paste(measure, collapse= " ~~collapse~~ ")
}))
} #first if
###END extract single text parts of the selected page range and store them in a matrix------------------------
measuresextracted <- list(measuresextracted)
names(measuresextracted) <- paste("measures_", names(textlist[t]), sep="")
textlist[[t]] <- c(textlist[[t]], measuresextracted)
#in order to save memory and speed up the following calculations the wholetext is removed
textlist[[t]][[1]] <- c("Full text removed in order to free memory and improve calculation performance. Please load again by readLines if the full text is needed.")
}  #end for outer loop - all files
####END extract the measures from each single text-------------------
##START - combine the Hannover files into one slot of the textlist----------------------
files.combine <- grep("GER__Hannover__", names(textlist))
#only do something if there are more than one list entries with the same entity name
if (length(files.combine ) > 1) {
#initial the combined list with the first element of elements to be combined
case.combine <- textlist[files.combine[1]]
#get the number of first list levels which serve as loop variable
num.listelements <- length(case.combine[[1]])
#loop through all elements to combine except the intial one
add.cases <- files.combine[-c(1)]
for (t in add.cases) {
for (u in 1:num.listelements) {
case.combine[[1]][[u]] <- c(case.combine[[1]][[u]], textlist[[t]][[u]])
}
}
#reassign the names
names(case.combine) <- "GER__Hannover__combined_files.txt"
names(case.combine[[1]]) <- c("GER__Hannover__combined_files.txt",
"pattern_GER__Hannover__combined_files.txt",
"measures_GER__Hannover__combined_files.txt")
#add the combined files list and delete the single files
textlist <- c(textlist,case.combine)
textlist <- textlist[-c(files.combine)]
}
##END- combine the Hannover files into one slot of the textlist------------------
time.elapsed <- rbind(time.elapsed, proc.time())
row.names(time.elapsed)[nrow(time.elapsed)] <- "after_text_loading_and_splitting"
#------------------------07
#####Purpose of the script: build a list of unique words within the whole set of text chunks stored in the list
#textlist structure is as follows
#name
#[[ENT]]             list ENTity (=document name)
#[[ENT]][[1-WT]]     Dummy sentence (before: Whole Text of the document including code at filehead)
#[[ENT]][[2-AEXP]]   Available EXtraction Pattern in the wholetext, matrix
#[[ENT]][[3-MEAS]]   separated single measures (including linebreaks marked with "~~collapse~~")
#[[ENT]][[4-UnWinM]] Unique Words in the single Measures, words which exist in the available wordlists have an E~X~T at the beginning
#str(textlist)
#text <- textlist[[24]][[2]]
#START read in available word list data---------------------------------------
##START read word lists---------------------------------------
#read all txt files in folder and build a list of their names
#then do some preliminary cleaning (trim, remove comments, etc.)
setwd(paste(wd.wordlists, "wordlists_for_analysis/", sep=""))
dir_current_root <- paste(getwd(), "/", sep="")
dirs <- list.dirs()
#the root dir is included as a single dot and is excluded, also leading dots are deleted and a final slash is added
dirs <- dirs[grep("[[:alpha:]]", dirs)]
dirs <- paste(gsub("./", "", dirs, fixed=TRUE),"/" , sep="")
#initialize list
wordlists <- vector(mode = "list", length = length(dirs))
for (f in 1:length(dirs)) {
setwd(paste(dir_current_root, dirs[f], sep=""))
files <- list.files(pattern=".txt")
#initial
text <- c("")
if( length(files) > 0) {
for (t in 1:length(files)) {
text <- c(text, readLines(files[t]))
}
}
#remove comments
text <- gsub("^.*#.*$", "", text)
text <- gsub("^.*~~.*$", "", text)
text <- gsub("^[[:blank:]]+", "", text)
text <- gsub("[[:blank:]]+$", "", text)
text <- text[text != " "]
text <- text[text != ""]
text <- Correct_misint_chars(text)
###START order the wordlist by number of blanks------------------------------
#which is later important for order of finding/replacing matches
text <- gsub("[[:blank:]]{2,}", " ", text)
text <- unique(text)
order.numblanks <- order(sapply(regmatches(text, gregexpr("[[:blank:]]", text)), length), decreasing=TRUE)
text <- text[order.numblanks]
###END order the worlist by number of blanks------------------------------
###START create wordlistnames on basis of names of the directory--------------------
category <- gsub("[\\W]", "", dirs[f], perl=TRUE)
category <- gsub("[[:blank:]]", "_", category)
listname <- category
###END create wordlistnames on basis of names of the directory--------------------
#add a number to the wordlist in case any names are duplicates
listname <- paste("c",f,"__",listname,sep="")
#if the list only contains one single word (i.e. the category in the first line) this
#causes problems in later steps, therefore dummy words are introduced
if(length(text) <= 1) text <- c(text, c("DUMMYUPPER", "dummylower", "Dummymixed"))
names(wordlists)[f] <- listname
wordlists[[f]] <- text
}
